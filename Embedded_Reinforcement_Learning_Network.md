# ğŸ’¡ Embedded Reinforcement Learning Framework: **Lux-Temp** ğŸŒ¡ï¸
**Adaptive On-Device Control for Light & Temperature Optimization**

ğŸ“¦ **Repository:**  
https://github.com/kris2475/Embedded_Reinforcement_Learning_Framework_Lux_Temp

---

## ğŸŒŸ What is Embedded Reinforcement Learning (RL)?

Traditional embedded systems rely on static, rule-based logic  
(e.g., â€œIf Temp > 75Â°F, turn on fanâ€).

**Reinforcement Learning (RL)** replaces rigid rules with *adaptive behavior*.  
An **agent** (the deviceâ€™s control logic) interacts with an **environment**  
(the physical world). It takes actions, receives rewards or penalties, and  
learns a **policy**â€”a strategy that maps states to optimal actions over time.

- **Policy (Ï€):** Mapping from observed states â†’ actions  
- **Value Function:** Predicts long-term reward (Q-values or V-values)

**Embedded RL** brings these capabilities directly to constrained hardware  
(MCUs, SBCs), enabling devices to adapt locally with minimal cloud dependence.

**Lux-Temp** applies Embedded RL to optimize environmental comfort  
(Lux + Temp) while minimizing energy consumptionâ€”problems too dynamic for  
traditional rule-based systems.

---

## â›°ï¸ The Core Challenge: RL on the Edge

| Constraint Category | Impact |
|--------------------|--------|
| âš¡ **Power** | MCU must keep inference ultra-fast to minimize active CPU time. |
| ğŸ’¾ **Memory** | Only KBâ€“small MB available â†’ tiny policy networks required. |
| â±ï¸ **Latency** | Control loops require microsecond-level execution. |
| ğŸ“‰ **Data Input** | Low-bandwidth sensors demand high sample efficiency. |

---

## ğŸ”„ The RL Loop: Why MCUs Struggle

1. **Observation Processing** â€” Sensor readings (Lux, Temp) â†’ normalized state vectors.  
2. **Policy Inference** â€” Most computationally expensive (many MAC operations).  
3. **Experience Storage** â€” Store (State, Action, Reward, Next State) tuples.  
4. **Policy Update** â€” Gradient descent & backprop (very compute-heavy).

This motivates **quantization**, **compression**, and **minimalist RL algorithms**.

---

## ğŸ§  Adapting RL for Embedded Devices

### 1ï¸âƒ£ Tabular Q-Learning â€“ Minimalist RL

- Stores Q-values in a simple table.  
- Decision = pick action with highest Q-value.  
- Occasional random action via Îµ-greedy exploration.

**Update Rule:**
```
Q(s,a) â† Q(s,a) + Î± [ r + Î³ max_aâ€™ Q(sâ€™,aâ€™) âˆ’ Q(s,a) ]
```

âœ“ Ultra-low memory  
âœ“ Integer math only  
âœ“ Fastest possible RL on MCUs  

---

### 2ï¸âƒ£ Neural Network Compression & Pruning

- Shrink network width/depth  
- Remove low-impact weights  
- Reduce MAC operations + RAM usage  

---

### 3ï¸âƒ£ Quantization â€“ MCU Efficiency Booster

Converts FP32 â†’ INT8.

**Linear quantization:**
```
r â‰ˆ S (q âˆ’ Z)
```

Enables fast integer-only inference.

Handled via **Quantization-Aware Training (QAT)** to minimize accuracy loss.

---

### 4ï¸âƒ£ Training Strategies

| Mode | Benefits | Drawbacks |
|------|----------|-----------|
| **Off-Device Training** | Minimal MCU load | No long-term adaptation |
| **On-Device Training** | Lifelong learning | High RAM/power demands |

---

## ğŸ’» Lux-Temp Framework Architecture

### ğŸ§© Components

#### **1. RL Agent Library**
Tiny DQN / Tabular Q-Learning optimized for embedded devices.

#### **2. Sensor Abstraction Layer**
Reads & normalizes Lux/Temp sensors â†’ quantized state vector.

#### **3. Inference Engine**
Integer-optimized policy execution (TF-Lite Micro or custom).

#### **4. Control Actuator Interface**
Translates actions â†’ PWM, relays, dimming controls, etc.

---

## ğŸ¯ Result

Lux-Temp enables:

- Adaptive, efficient comfort control  
- Energy-optimized decision-making  
- Real-time inference on low-power microcontrollers  
- Local learning without the cloud  
