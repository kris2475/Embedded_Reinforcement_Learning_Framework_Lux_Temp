üí° REINFORCEMENT LEARNING ADAPTIVE CLIMATE CONTROL UNIT (ACCU)

Project Overview

This project implements a Tabular Q-Learning agent designed to manage the climate (Illuminance and Temperature) within a confined space. The primary objective is to learn an energy-efficient policy that maintains user comfort while minimizing actuator activity.

The solution is divided into a Python Simulation for rapid training and policy development, and an Embedded C++ Sketch for real-time execution of the learned policy on constrained hardware (e.g., Arduino/ESP32).

üß† Reinforcement Learning Space

The State and Action spaces are strictly defined and mirrored between the simulation and embedded code to ensure policy portability.

1. State Space ($\mathbf{S}$): 9 Discrete States

The environment is discretized into a 3x3 grid based on sensor readings from a BH1750 (Lux) and a BMP180 (Temperature).

Environment Factor

Bin $\mathbf{0}$ (Low)

Bin $\mathbf{1}$ (Target)

Bin $\mathbf{2}$ (High)

Illuminance (Lux)

$\le 100 \text{ lx}$

$101 - 499 \text{ lx}$

$\ge 500 \text{ lx}$

Temperature ($\text{}^\circ\text{C}$)

$\le 18.0 \text{}^\circ\text{C}$

$18.1 - 23.9 \text{}^\circ\text{C}$

$\ge 24.0 \text{}^\circ\text{C}$

The Target Comfort Zone is State 4 (S4): Medium Lux (Bin 1) and Comfort Temp (Bin 1).

2. Action Space ($\mathbf{A}$): 5 Discrete Actions

Actions correspond directly to actuator controls, with an explicit Energy Cost integrated into the reward function.

Action ID

Action Name

Energy Cost / Reward

0

LIGHT+

Negative Reward Penalty (-0.05)

1

LIGHT-

Negative Reward Penalty (-0.05)

2

TEMP+

Negative Reward Penalty (-0.05)

3

TEMP-

Negative Reward Penalty (-0.05)

4

IDLE

Small Reward Bonus (+0.01)

üõ†Ô∏è Implementation Details

Embedded C++ Sketch (lux_rl_framework.ino)

The sketch runs the Q-Learning loop, interacts with physical sensors (BH1750, BMP180), and outputs actuator commands via a Serial proxy.

Crucial Moment Logging:
To aid debugging and analysis on constrained hardware, the sketch implements logging for significant events:

Exploration: Random actions taken under the $\epsilon$-Greedy strategy.

Crucial Update: Q-Table updates where the Temporal Difference (TD) Error magnitude $\ge 1.0$, indicating a high-impact learning event.

Policy Change: When an update causes the "best" action for a given state to switch.

Policy Transfer

In deployment, the Q-Table generated by the Python simulation is extracted and hardcoded into the C++ sketch, allowing the embedded system to immediately execute the trained, optimized policy without the need for slow, on-chip training.

üìä Simulation Results Summary

The Python simulation ran for 200 episodes using $\alpha=0.1$, $\gamma=0.9$, and $\epsilon=0.3$.

Key Findings

Metric

Value

Total Episodes

200

Crucial Updates (TD Error $\ge 1.0$)

20

Policy Changes (Best Action Switch)

10

Final Policy Analysis (State S4)

The primary goal of the reward structure was to train the agent to select the IDLE action when in the Target Comfort Zone (S4), thereby maximizing energy efficiency.

State ID

State Description

Best Action (Learned Policy)

S4

Target Zone (Medium Lux, Comfort Temp)

LIGHT+

Conclusion on Energy Efficiency (S4 Failure):
The agent's learned policy for the target comfort state (S4) is LIGHT+ instead of the desired IDLE. This outcome suggests that the current reward structure needs tuning. Specifically, the energy bonus for the IDLE action ($\mathbf{+0.01}$) was insufficient to overcome the potential long-term benefits or penalties associated with environmental drift, forcing the agent to continuously intervene with active, energy-consuming actions.

Proposed Next Steps:

Increase the IDLE reward bonus (e.g., to $+0.05$ or higher).

Increase the penalty for active actions.

Run the Python simulation for a greater number of episodes to ensure full convergence.

Re-evaluate the environmental drift model in the simulation to better match real-world fluctuations.
