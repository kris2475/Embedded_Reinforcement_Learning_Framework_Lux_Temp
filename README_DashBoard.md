This project implements a complete Q-Learning framework on an ESP32, coupled with a real-time, graphical web dashboard. The purpose is to train an agent to maintain optimal environmental conditions (Temperature and Illuminance/Lux) using a small set of predefined actions, while minimizing energy expenditure. 1. System Overview and Core Concept The system operates as an Adaptive Climate Control Unit (ACCU) using Reinforcement Learning (RL). Goal: Maintain the environment at a Target Lux of 300 and a Target Temperature of 21°C. Method: Q-Learning, a model-free RL algorithm, is used to learn the best action to take in every environmental state. Hardware: An ESP32 is used to run the RL algorithm, connect to Wi-Fi, and host the web server for the dashboard. Sensors: Simulated or actual BH1750 (Lux) and BMP180 (Temperature) sensors provide the environment's current state. Q-Learning Cycle (Every 5 Seconds) Sense (S): Read current Lux and Temp. Convert these continuous values into a discrete State Index (S0 to S8). Reward (R): Calculate a reward value based on how close the current conditions are to the targets, penalizing actions (energy cost). Learn (Q-Update): Update the Q-Table entry for the previous state/action pair using the Temporal Difference (TD) error equation: $$Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma \max_{A'} Q(S', A') - Q(S, A)]$$ Action (A'): Choose the next action (A') using an Epsilon-Greedy strategy (exploit the best-known action most of the time, explore randomly occasionally). Execute: Apply the action (simulated actuator control). 2. The Dashboard and Streaming Fix The dashboard is a large, single-file HTML/CSS/JavaScript application embedded directly into the Arduino sketch's Flash Memory (PROGMEM). Crucial Technical Fix (sendLargeHtmlFromProgmem) The large size of the dashboard HTML often causes memory allocation failures or watchdog timer resets when the ESP32 tries to buffer the entire file in RAM before sending. To solve the "still blank" issue, the final sketch implements a low-level, robust streaming solution: The function sendLargeHtmlFromProgmem reads the HTML content directly from Flash (PROGMEM). It sends the HTTP header with the correct Content-Length. It then iterates through the content, reading and sending the file in small, safe 512-byte chunks to the client. This bypasses the memory-heavy internal buffers of the high-level web server functions. Expected behavior in Serial Monitor: When a browser connects, you should see the message: HTML stream complete. Connection closed. 3. Interpreting the Dashboard The dashboard provides a real-time visualization of the system's learning and performance. A. Sensor Data & Status Element Interpretation Lux / Temp Value The current, raw sensor readings. Gauges Visual representation of the sensor value relative to the target. The center line is the target (300 Lux / 21°C). Reward Value The immediate reward received in the last step. (Range: -1.0 to +1.0). Higher is better. State Index (S0-S8) The discrete environmental state the system currently recognizes. Action Taken The actuator command chosen by the RL agent in the last step (e.g., LIGHT+, TEMP-). B. State Space Mapping (S0 to S8) The continuous Lux and Temp readings are converted into 9 distinct states based on three bins for each variable: State Index (S) Lux Bin Temp Bin Description S0 Low (< 100) Cold (< 18°C) Environment is too dark and too cold. S4 Medium/Target (100-500) Comfort/Target (18-24°C) Ideal state (Target). S8 High (> 500) Hot (> 24°C) Environment is too bright and too hot. C. Q-Table Policy Heatmap (The Core of the Learning) This table shows the agent's learned knowledge: Feature Interpretation Rows Represent the 9 possible environmental states (S0 to S8). Columns Represent the 5 available actions (LIGHT+, LIGHT-, TEMP+, TEMP-, IDLE). Cell Value (Color) This is the Q-Value. It represents the expected cumulative future reward if the agent takes that specific Action (column) while in that specific State (row). Color Scale Green (High Positive Q-Value) = Action is consistently good. Red (High Negative Q-Value) = Action is consistently bad. Blue Star (★) This marks the Greedy Action for that state—the action the agent believes is currently the best choice based on the highest Q-Value. This defines the current policy. Active State The entire row corresponding to the current state ($S_{current}$) is highlighted with a pulsing blue outline. D. RL Log The log provides crucial insight into the learning dynamics: "EXPLORE:" Indicates the agent deliberately chose a random action (Epsilon-Greedy). "EXPLOIT:" Indicates the agent chose the action with the highest known Q-value (Greedy). "LEARN: ... [CRUCIAL UPDATE]" Indicates the TD Error was high, meaning the observed reward was very surprising, leading to a large update in the Q-Table. "LEARN: ... [POLICY CHANGE: XXX]" Indicates that as a result of the update, the best known action (the Star) for a state has changed. This is the moment the agent truly learns something new.
