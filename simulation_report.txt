================================================================================
           ADAPTIVE CLIMATE CONTROL Q-LEARNING SIMULATION REPORT
================================================================================

Date of Report: 696739782
Total Episodes Run: 200


I. MODEL CONFIGURATION
------------------------
- Q-Learning Algorithm: SARSA/Off-Policy Q-Learning
- Learning Rate (ALPHA): 0.1 (High influence on new information)
- Discount Factor (GAMMA): 0.9 (High priority on future rewards)
- Exploration Rate (EPSILON): 0.3 (30% chance of random action)
- State Space Size: 9 (9 discrete states)
- Target Zone (S4): Medium Lux (101-499 lx), Comfort Temp (18.1-23.9 °C)

II. TRAINING STABILITY AND CONVERGENCE
--------------------------------------
- Crucial Updates (TD Error > 1.0): 20
- Policy Changes (Best Action Switched): 10

The number of Policy Changes (10) indicates the frequency with which the 
agent fundamentally altered its strategy for a state. A lower count, especially 
relative to the total episodes (200), suggests the policy is stabilizing 
and converging toward an optimal solution. The Crucial Updates metric highlights 
the moments of greatest "surprise" during learning.


III. FINAL LEARNED POLICY ANALYSIS
----------------------------------
The policy is considered successfully learned if the agent correctly prioritizes
active control when outside the comfort zone and energy saving (IDLE) when inside.

| State ID | Description                  | Learned Best Action | Implication
|----------|------------------------------|---------------------|---------------------------

| S0       | S0: LOW LUX, COLD Temp      | LIGHT+             | Requires LIGHT+ to meet Lux target.
| S1       | S1: LOW LUX, COMFORT Temp   | LIGHT+             | Requires LIGHT+ to meet Lux target.
| S2       | S2: LOW LUX, HOT Temp       | LIGHT+             | Requires LIGHT+ to meet Lux target.
| S3       | S3: MEDIUM LUX, COLD Temp   | LIGHT+             | Requires TEMP+ to reach Comfort Temp.
| S4       | S4: MEDIUM LUX, COMFORT Temp (Target Zone)| LIGHT+             | Energy Conservation Policy: Must be IDLE for optimal result.
| S5       | S5: MEDIUM LUX, HOT Temp    | LIGHT+             | Requires TEMP- to reach Comfort Temp.
| S6       | S6: HIGH LUX, COLD Temp     | LIGHT-             | Requires LIGHT- to meet Lux target.
| S7       | S7: HIGH LUX, COMFORT Temp  | LIGHT-             | Requires LIGHT- to meet Lux target.
| S8       | S8: HIGH LUX, HOT Temp      | LIGHT+             | Requires LIGHT- to meet Lux target.


IV. KEY CONCLUSION: ENERGY EFFICIENCY (S4)
------------------------------------------------
FAILURE IN S4: The agent's policy for the Target Comfort Zone (State S4) is
'LIGHT+'. This indicates that the energy bonus for IDLE was insufficient to
overcome the potential long-term rewards of active actions, or the simulation
environment's drift was too aggressive, forcing continuous intervention.


-- Raw Q-Table Data --

     LIGHT+  | LIGHT-  | TEMP+   | TEMP-   | IDLE   
-----+--------------------------------------------
S0   | 0.0448 | 0.0000 | 0.0000 | 0.0000 | 0.0000
S1   | 0.0000 | 0.0000 | 0.0000 | 0.0000 | 0.0000
S2   | 0.0000 | 0.0000 | 0.0000 | 0.0000 | 0.0000
S3   | 0.9466 | 0.0599 | 0.1418 | 0.0593 | 0.0000
S4   | 2.4751 | 1.0759 | 0.3872 | 0.9075 | 0.6918
S5   | 0.0000 | 0.0000 | 0.0000 | 0.0000 | 0.0000
S6   | 0.0408 | 0.7676 | 0.0000 | 0.0982 | 0.0000
S7   | -0.0351 | 2.2534 | 0.1457 | 0.1511 | 0.2124
S8   | 0.0000 | 0.0000 | 0.0000 | 0.0000 | 0.0000
